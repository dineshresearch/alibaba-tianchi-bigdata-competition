{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime as dt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loading original data\n",
    "shopInfoFile = '../dataset/shop_info.txt'\n",
    "\n",
    "shopInfo = pd.read_table(shopInfoFile, sep = ',', header = None)\n",
    "shopInfo.columns = ['shopID', 'city', 'locationID', 'perPay', 'score', 'commentCnt', 'shopLevel', 'cate1', 'cate2', 'cate3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load training and testing data\n",
    "payTH = pd.read_csv('../preprocess/payTH_parallel.txt', sep=\" \", header = None)\n",
    "trainFile = '../preprocess/trainValidFeatures_ensemble.csv'\n",
    "testFile = '../preprocess/validFeatures_ensemble.csv'\n",
    "trainData = pd.read_csv(trainFile, header = None)\n",
    "testData = pd.read_csv(testFile, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preparing training set and validation set\n",
    "periods = [7, 14, 28, 56, 112]\n",
    "stats = ['mean', 'std', 'skew', 'kurtosis']\n",
    "recentDataColumns = []\n",
    "for period in periods:\n",
    "    for stat in stats:\n",
    "        column  = 'last' + str(period) + 'days_' + stat\n",
    "        recentDataColumns.append(column)\n",
    "\n",
    "periods = [7, 14, 28]\n",
    "stats = ['meanView', 'stdView', 'skewView', 'kurtosisView']\n",
    "recentDataViewColumns = []\n",
    "for period in periods:\n",
    "    for stat in stats:\n",
    "        column = 'last' + str(period) + 'days_' + stat\n",
    "        recentDataViewColumns.append(column)            \n",
    "        \n",
    "periods = [7, 14, 28, 56, 112]\n",
    "trends = ['copy', 'ridge']\n",
    "currentTrendcolumns = []\n",
    "for period in periods:\n",
    "    for trend in trends:\n",
    "        column = 'last' + str(period) + 'days_' + trend\n",
    "        currentTrendcolumns.append(column)\n",
    "        \n",
    "primaryKey = ['shopID', 'year', 'month', 'day']\n",
    "columnDic = {\n",
    "    'basicInfo':['city', 'perPay', 'score', 'commentCnt', 'shopLevel', 'category'],\n",
    "    'recentData':recentDataColumns,\n",
    "    'recentDataView':recentDataViewColumns,\n",
    "    'currentTrend':currentTrendcolumns,\n",
    "    'temporalInfo':['dayOfWeek', 'holiday', 'numHolidayLast', 'numHolidayCur', 'numHolidayNext'],\n",
    "    'weather':['maxTemp', 'minTemp', 'weather', 'pm']\n",
    "}\n",
    "\n",
    "ensembleCol = ['shopID', 'year', 'month', 'day']\n",
    "orderCol = ['basicInfo', 'recentData', 'temporalInfo', 'currentTrend', 'weather', 'recentDataView']\n",
    "for col in orderCol:\n",
    "    ensembleCol = ensembleCol + columnDic[col]\n",
    "    \n",
    "trainData.columns = ensembleCol\n",
    "testData.columns = ensembleCol\n",
    "\n",
    "startDateTrain = dt.date(2016, 9, 20)\n",
    "endDateTrain = dt.date(2016, 10, 17)\n",
    "startDateTest = dt.date(2016, 10, 18)\n",
    "endDateTest = dt.date(2016, 10, 31)\n",
    "startDate = dt.date(2015, 7, 1)\n",
    "endDate = dt.date(2016, 10, 31)\n",
    "\n",
    "startTrain = (startDateTrain - startDate).days\n",
    "endTrain = (endDateTrain - startDate).days\n",
    "startValid = (startDateTest - startDate).days\n",
    "endValid = (endDateTest - startDate).days\n",
    "\n",
    "trainLabel = payTH[np.arange(startTrain, endTrain + 1)].values.reshape(1, -1)[0]\n",
    "validLabel = payTH[np.arange(startValid, endValid + 1)].values.reshape(1, -1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def detectNaN(a):\n",
    "    for i in range(len(a[0])):\n",
    "        e = True\n",
    "        for j in range(len(a) - 1):\n",
    "            if np.isnan(a[j][i]):\n",
    "                e = False\n",
    "                break\n",
    "        if (not e):\n",
    "            print(i)\n",
    "def replace(a):\n",
    "    for i in range(len(a[0])):\n",
    "        e = True\n",
    "        for j in range(len(a)):\n",
    "            if np.isnan(a[j][i]):\n",
    "                a[j][i] = a[j - 1][i]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preprocessing training set\n",
    "trainDataArray = np.array(trainData)\n",
    "trainDataArrayProcessed = np.delete(trainDataArray, [1, 2], 1)\n",
    "trainDataProcessed = replace(trainDataArrayProcessed)\n",
    "detectNaN(trainDataProcessed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(trainDataProcessed)\n",
    "trainDataNormalized = scaler.transform(trainDataProcessed)\n",
    "detectNaN(trainDataNormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preprocessing validation set\n",
    "testDataArray = np.array(testData)\n",
    "testDataArrayProcessed = np.delete(testDataArray, [1, 2], 1)\n",
    "testDataProcessed = replace(testDataArrayProcessed)\n",
    "detectNaN(testDataProcessed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(testDataProcessed)\n",
    "testDataNormalized = scaler.transform(testDataProcessed)\n",
    "detectNaN(testDataNormalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.15025776744\n",
      "5 0.135792059521\n",
      "10 0.131294982587\n",
      "20 0.129674811501\n",
      "50 0.131017965476\n"
     ]
    }
   ],
   "source": [
    "neighbors = [2, 5, 10, 20, 50]\n",
    "preds_knn = []\n",
    "eval_knn = []\n",
    "recordNum = len(validLabel)\n",
    "\n",
    "for num in neighbors:\n",
    "    \n",
    "    rgs_knn = KNeighborsRegressor(n_neighbors=num)\n",
    "    rgs_knn.fit(trainDataNormalized, trainLabel)\n",
    "    pred_knn = rgs_knn.predict(testDataNormalized)\n",
    "    preds_knn.append(pred_knn)\n",
    "    evaluation = abs((validLabel - pred_knn)/(validLabel + pred_knn)).sum()/recordNum\n",
    "    eval_knn.append(evaluation)\n",
    "    print(num, evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
